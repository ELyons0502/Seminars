Moving beyond linearity
import pandas as pd 
import numpy as np 
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd 
import numpy as np 
import matplotlib as mpl
import matplotlib.pyplot as plt
df = pd.read_csv('C:/Users/ELyons/Data/Wage.csv')
df.head(5)
year	age	maritl	race	education	region	jobclass	health	health_ins	logwage	wage
0	2006	18	1. Never Married	1. White	1. < HS Grad	2. Middle Atlantic	1. Industrial	1. <=Good	2. No	4.318063	75.043154
1	2004	24	1. Never Married	1. White	4. College Grad	2. Middle Atlantic	2. Information	2. >=Very Good	2. No	4.255273	70.476020
2	2003	45	2. Married	1. White	3. Some College	2. Middle Atlantic	1. Industrial	1. <=Good	1. Yes	4.875061	130.982177
3	2003	43	2. Married	3. Asian	4. College Grad	2. Middle Atlantic	2. Information	2. >=Very Good	1. Yes	5.041393	154.685293
4	2005	50	4. Divorced	1. White	2. HS Grad	2. Middle Atlantic	2. Information	1. <=Good	1. Yes	4.318063	75.043154
from sklearn.preprocessing import PolynomialFeatures 
​
X4 = PolynomialFeatures(4).fit_transform(df['age'].values.reshape(-1,1))
# Here, we first select predictor of wage - 'age'. 
# We then choose the power to raise the age by - '4'
# .values.reshape is used. The .values helps to reshape the vector of age (i.e., age in a table going down), into a vector 
# (or list) of 18, 24, 45...
# Can be seen with: df(['age'].values for clarity
# The .reshape aspect is required simply to reorder the list as a column. Look at code below for clarity.
​
#For example:
df['age']
​
0       18
1       24
2       45
3       43
4       50
        ..
2995    44
2996    30
2997    27
2998    27
2999    55
Name: age, Length: 3000, dtype: int64
df['age'].values
array([18, 24, 45, ..., 27, 27, 55], dtype=int64)
df['age'].values.reshape(-1,1)
array([[18],
       [24],
       [45],
       ...,
       [27],
       [27],
       [55]], dtype=int64)
X4 = PolynomialFeatures(4).fit_transform(df['age'].values.reshape(-1,1))
import statsmodels.api as sm
​
fit2 = sm.GLS(df['wage'], X4).fit()
fit2.summary().tables[1]
coef	std err	t	P>|t|	[0.025	0.975]
const	-184.1542	60.040	-3.067	0.002	-301.879	-66.430
x1	21.2455	5.887	3.609	0.000	9.703	32.788
x2	-0.5639	0.206	-2.736	0.006	-0.968	-0.160
x3	0.0068	0.003	2.221	0.026	0.001	0.013
x4	-3.204e-05	1.64e-05	-1.952	0.051	-6.42e-05	1.45e-07
Not entirely sure how to interpret this, but essentially can see the decreasing explanatory strenght of age as power increases

Next, let us consider the task of predicting whether an individual earns more than £250,000

# Create a response matrix 
​
y = (df['wage'] > 250).map({False:0, True:1}) # we essentially assign a dummy value depending on whether wage > 250 or not
y.head(10) # just to quickly show 
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-a2f7d428fa56> in <module>
      1 # Create a response matrix
      2 
----> 3 y = (df['wage'] > 250).map({False:0, True:1}) # we essentially assign a dummy value depending on whether wage > 250 or not
      4 y.head(100) # just to quickly show

NameError: name 'df' is not defined

# Now fit a logistic model, as have re-classified using dummy v
​
clf = sm.Logit(y, X4)
res = clf.fit()
​
Optimization terminated successfully.
         Current function value: 0.116870
         Iterations 12
# Generate a sequence of age values spanning the range 
​
age_grid = np.arange(df['age'].min(),df['age'].max()).reshape(-1,1) # here, we are simply creating a list of ages from data
age_grid 
array([[18],
       [19],
       [20],
       [21],
       [22],
       [23],
       [24],
       [25],
       [26],
       [27],
       [28],
       [29],
       [30],
       [31],
       [32],
       [33],
       [34],
       [35],
       [36],
       [37],
       [38],
       [39],
       [40],
       [41],
       [42],
       [43],
       [44],
       [45],
       [46],
       [47],
       [48],
       [49],
       [50],
       [51],
       [52],
       [53],
       [54],
       [55],
       [56],
       [57],
       [58],
       [59],
       [60],
       [61],
       [62],
       [63],
       [64],
       [65],
       [66],
       [67],
       [68],
       [69],
       [70],
       [71],
       [72],
       [73],
       [74],
       [75],
       [76],
       [77],
       [78],
       [79]])
# Generate test data 
​
X_test = PolynomialFeatures(4).fit_transform(age_grid)
# Predict the value of the generated ages 
pred1 = fit2.predict(X_test) # salary for a given age from age_grid
pred2 = res.predict(X_test) # Pr(wage > 250) for a given age from age_grid
fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,5))
fig.suptitle('Polynomial to the power of 4', fontsize=14)
​
ax1.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.3) # The alpha here sets the darkness of dots on left panel
​
ax1.plot(age_grid, pred1, color = 'blue')
ax1.set_ylim(ymin=0)
​
​
#Logistic regression showing Pr(wage>250)
ax2.plot(age_grid, pred2, color ="b")
ax2.scatter(df.age, y/5, s=30, c='grey', marker='|', alpha=0.4) # c= grey gives colour of title. Not sure what y does
​
ax2.set_ylim(-0.01,0.21) # sets the range of y axis on panel 2
ax2.set_xlabel('age')
ax2.set_ylabel('Pr(wage >250)')
Text(0, 0.5, 'Pr(wage >250)')

Step functions
# Here, we are going to create clusters of data and create a model for each
# To do this, use cut function
df_cut, bins = pd.cut(df['age'],4, retbins = True, right = True) 
# Note, the 'bins' are the four clusters of data we are creating. We are cutting the variable 'age'
# Retbins  simply means to show the different groups (17.938-33.5 etc shown below)
# Right is required to show you want all elements
​
df_cut.value_counts(sort=False)
# The value counts shows the elements of data in each bin
(17.938, 33.5]     750
(33.5, 49.0]      1399
(49.0, 64.5]       779
(64.5, 80.0]        72
Name: age, dtype: int64
df_steps = pd.concat([df['age'], df_cut, df.wage], keys = ['age', 'age_cuts', 'wage'], axis = 1)
# Here, we are simply creating a table for each data point. We have asked for the age and wage to be shown, as well as the 
# age cut (i.e., group cluster) that that person beongs to given age
​
df_steps
age	age_cuts	wage
0	18	(17.938, 33.5]	75.043154
1	24	(17.938, 33.5]	70.476020
2	45	(33.5, 49.0]	130.982177
3	43	(33.5, 49.0]	154.685293
4	50	(49.0, 64.5]	75.043154
...	...	...	...
2995	44	(33.5, 49.0]	154.685293
2996	30	(17.938, 33.5]	99.689464
2997	27	(17.938, 33.5]	66.229408
2998	27	(17.938, 33.5]	87.981033
2999	55	(49.0, 64.5]	90.481913
3000 rows × 3 columns

# Create dummy variables for the age groups 
​
df_steps_dummies = pd.get_dummies(df_steps['age_cuts'])
# Hewre, we are ascribing 1 into the age cut that each datapoint (0...2999) belongs to. Data point 0 = age 18, so gropup 1
df_steps_dummies
(17.938, 33.5]	(33.5, 49.0]	(49.0, 64.5]	(64.5, 80.0]
0	1	0	0	0
1	1	0	0	0
2	0	1	0	0
3	0	1	0	0
4	0	0	1	0
...	...	...	...	...
2995	0	1	0	0
2996	1	0	0	0
2997	1	0	0	0
2998	1	0	0	0
2999	0	0	1	0
3000 rows × 4 columns

df_steps_dummies = sm.add_constant(df_steps_dummies)
df_steps_dummies
const	(17.938, 33.5]	(33.5, 49.0]	(49.0, 64.5]	(64.5, 80.0]
0	1.0	1	0	0	0
1	1.0	1	0	0	0
2	1.0	0	1	0	0
3	1.0	0	1	0	0
4	1.0	0	0	1	0
...	...	...	...	...	...
2995	1.0	0	1	0	0
2996	1.0	1	0	0	0
2997	1.0	1	0	0	0
2998	1.0	1	0	0	0
2999	1.0	0	0	1	0
3000 rows × 5 columns

When using dummy variables like above, there is an issue of multicollinearity. To get round this, we need to remove one of the groups.

# Here, we will drop first cluster
​
df_steps_dummies = df_steps_dummies.drop(df_steps_dummies.columns[1], axis = 1)
df_steps_dummies.head(5)
​
# When we eventually run a model, we will essentially use the constant (y intercept) as the coef. for bin group 1
# The coef for the remaining groups represents the wage change RELATIVE to the first group
​
const	(33.5, 49.0]	(49.0, 64.5]	(64.5, 80.0]
0	1.0	0	0	0
1	1.0	0	0	0
2	1.0	1	0	0
3	1.0	1	0	0
4	1.0	0	1	0
fit3 = sm.GLM(df_steps.wage, df_steps_dummies).fit()
fit3.summary().tables[1]
coef	std err	z	P>|z|	[0.025	0.975]
const	94.1584	1.476	63.790	0.000	91.265	97.051
(33.5, 49.0]	24.0535	1.829	13.148	0.000	20.468	27.639
(49.0, 64.5]	23.6646	2.068	11.443	0.000	19.611	27.718
(64.5, 80.0]	7.6406	4.987	1.532	0.126	-2.135	17.416
# Now, let's predice whether wage > 250 - as per before, but using these specific groups
​
​
​
y = (df['wage']>250).map({False:0, True:1})
​
clf2 = sm.Logit(y, df_steps_dummies)
res2 = clf2.fit()
​
pred3 = res2.predict(X_test2)
